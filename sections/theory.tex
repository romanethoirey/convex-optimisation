\documentclass[../convex_optimization.tex]{subfiles}
\theoremstyle{definition} \newtheorem{defi}{Def}
\theoremstyle{definition} \newtheorem{prop}{Prop}
\begin{document}
This chapter will serve to introduce the concept of \ldots

TODO expand

\subsection{Problem formulation}
A convex optimization problem consists of, given a convex function
$f: S \to \mathbb R$ for a convex set $S$, finding
a point $x_0 \in S$ which minimizes the value $f(x_0) \in \mathbb R$.

Recall the definitions for convex sets and functions:
A set $S \subset \mathbb R^n$ is a \textbf{convex set} iff
\begin{equation}
    \forall (x, y) \in S^2: (1-\lambda)x + \lambda y \in S,
    \forall \lambda \in [0, 1]
    \label{convex_set}
\end{equation}
interpreted as any point on the line segment between two points in the set,
is also in the set.
A function $f$ is a \textbf{convex function on $S$} iff
\begin{equation}
    \forall (x, y) \in S^2, \forall \lambda \in [0, 1]:
    f((1-\lambda)x + \lambda y) \leq (1-\lambda)f(x) + \lambda f(y)
    \label{convex_function}
\end{equation}
thus the function value of a point between two contained points must
not exceed the value over the same point in the linear combination between the two contained points.

We will also recall a corollary for the case $n=1$ 
that for a function of one variable,
twice differentiable on an interval $I$ of $\mathbf R$,
the function being \textbf{convex} is equivalent to its second derivative
being positive:
\begin{equation} \label{convex_corollary}
\begin{gathered}
    \text{For } f: \mathbb R \to \mathbb R
    \text{ with $f$ twice differentiable on the interval $I$}, \\
f \textbf{ convex}\text{ on } I \iff f''(x) \geq 0, x \in I
\end{gathered}
\end{equation}
When solving classical convex first-order or zero-order convex
optimization problems, these definition alone provide the solution.
For problems of second order or higher, these properties alone are
no longer sufficient.

\subsection{Matrix definition}
Assuming the existence of the second derivative, we introduce
the Hessian matrix as a mathematical object. We will use this
to solve second order convex optimization problems.
We define the Hessian matrix:
\begin{defi}
\begin{equation}
    \mathbf H = \begin{bmatrix}
        \dfrac{\partial^2 f}{\partial x_1^2} &
        \dfrac{\partial^2 f}{\partial x_1\,\partial x_2} &
        \cdots & \dfrac{\partial^2 f}{\partial x_1\,\partial x_n} \\[2.2ex]
        \dfrac{\partial^2 f}{\partial x_2\,\partial x_1} &
        \dfrac{\partial^2 f}{\partial x_2^2} & \cdots &
        \dfrac{\partial^2 f}{\partial x_2\,\partial x_n} \\[2.2ex]
        \vdots & \vdots & \ddots & \vdots \\[2.2ex]
        \dfrac{\partial^2 f}{\partial x_n\,\partial x_1} &
        \dfrac{\partial^2 f}{\partial x_n\,\partial x_2} &
        \cdots & \dfrac{\partial^2 f}{\partial x_n^2}
    \end{bmatrix}
    \label{hessian_matrix_definition}
\end{equation}
    \label{hessian_definition}
\end{defi}

This matrix of second derivatives can also be used as a linear
operator evaluated at a point $\vec{v}=(x_1, \ldots, x_n)^T$,
with $\mathbf H(\vec{v})$.

In the case of multivariate functions, the convexity of a function
implies properties of the Hessian and vice versa, which
helps to solve optimization problems:
\begin{prop}
\begin{gather}
    \text{For } f: S \subseteq \mathbb R^n \to \mathbb R,
    \text{C \textbf{convex} and open set and } f \in C^2,
    \text{ then}\nonumber\\
    f \text{ \textbf{convex} on }S \iff
    \mathbf H \text{ is \textbf{positive semidefinite} } \forall x \in S\label{hessian_convex}\\
    \text{and}\nonumber\\
    f \text{ \textbf{strictly convex} on S} \iff
    \mathbf H \text{ is \textbf{positive definite} } \forall x \in S
    \label{positive_definite}
\end{gather}
\end{prop}
\begin{defi}
\begin{gather}
    \text{A matrix $A$ is called \textbf{positive definite} if:}
    \nonumber\\
    \forall \vec{v} \neq 
    \begin{pmatrix} 0 \\ \vdots \\ 0
    \end{pmatrix}\colon
    \rlap{\ensuremath{\underbrace{\phantom{
                    {\aunderbrace[D]{\vec{v}}_{1\times n}}^T
                    \aunderbrace[D]{A}_{n\times n}
                    \aunderbrace[D]{\vec{v}}_{n\times 1}
    }}_{\in \mathbb R}}}
    {\aunderbrace[D]{\vec{v}}_{1\times n}}^T
    \aunderbrace[D]{A}_{n\times n}
    \aunderbrace[D]{\vec{v}}_{n\times 1}
    > 0
\end{gather}
\end{defi}
Similarly, a positive semidefinite matrix can be defined by relaxing the inequality in (\ref{positive_definite}):
\begin{defi}
\begin{gather}
    \text{A matrix $A$ is called \textbf{positive semidefinite} if:}
    \nonumber\\
    \forall \vec{v} \neq 
    \begin{pmatrix} 0 \\ \vdots \\ 0
    \end{pmatrix}\colon
    \vec{v}^T\space A\space \vec{v} \geq 0
\end{gather}
\end{defi}


Convexity is very important in optimization theory because it provides
additional properties that help to minimze more easily or quickly,
especially in $\mathbb R^n$. For example, with a convex function
a local minimum is known to be a global minimum:
\begin{gather}
    \text{For } f: S \subseteq \mathbb R^n \to \mathbb R,
    \text{ $f$ convex on a convex set S:}
    \nonumber\\
    \text{If } x^* \in S \text{ is a local minimum to $f$, } 
    x^* \text{ is also a global minimum to $f$.}\\
    \text{If $f$ is \textbf{strictly convex} and $x^*$ is a minimum,
    it is the \underline{only} global minimum.}
\end{gather}
\subsection{Convex Optimization}
Consider the case of unconstrained optimization of a 1-variable function.
We take the case of a function $f$ that is convex on a convex set $S$
with only one variable that we minimize without constraint:
\begin{align}
    \text{Objective: } &\quad\xi\colon \min_{x\in\mathbb R} \quad f(x)
    \nonumber\\
    \text{1:st order condition:} &\quad
    \text{If $x^*$ is a solution to $\xi$,
    then it must hold that $f'(x^*) = 0$}\label{firstcond}\\
    \text{2:nd order condition:} &\quad
    \text{Assume there exists a $x^*$ which verifies (\ref{firstcond}), then:}
    \nonumber\\& \qquad
    \begin{aligned}
        f''(x^*) > 0 &\Rightarrow & x^* \text{ is a local minimum}\\
        x^* \text{ local minimum} & \Rightarrow & f''(x^*) \geq 0\\
        x^* \text{ local maximum} & \Rightarrow & f''(x^*) \leq 0
    \end{aligned}\\
    &\quad
    \text{As $f$ is convex, } \forall x \in\mathbb R, f''(x) \geq 0
    \Rightarrow x^* \text{ is a global minimum}\nonumber
\end{align}

Moving on to the case of a multivariate convex function.
Let $f$ be a convex function of $n$ variables defined on $\mathbb R^n$.
We consider a minimization program:
\begin{align}
    \text{Objective:} &\quad\xi\colon \min_{x_1,\ldots,x_n}
    \quad f(x_1, \ldots, x_n)
    \nonumber\\
    \text{1:st order condition:} &\quad
    \text{If $\vec{x}^*=(x_1^*, \ldots, x_n^*)^T$ is a solution to $\xi$,
    then it must hold that }
    \nonumber\\& \qquad \qquad \quad
    \begin{aligned}
        \frac{\partial f(\vec{x}^*)}{\partial x_i} = 0\label{multifirst}
        \quad \forall i=1,\ldots , n
    \end{aligned}\\
    \text{2:nd order condition:} &\quad
    \text{Assume there exists a $\vec{x}^*$ which verifies (\ref{multifirst}), then:}
    \nonumber\\ & \qquad
    \begin{aligned}
        \mathbf H(\vec{x}^*) \text{ is \textbf{positive definite}}
        &\Rightarrow & \vec{x}^*\text{ is a local minimum}\\
        \mathbf H(\vec{x}^*) \text{ is \textbf{negative definite}}
        &\Rightarrow & \vec{x}^*\text{ is a local minimum}\\
        f''(x^*) > 0 &\Rightarrow & x^* \text{ is a local maximum}
    \end{aligned}\\
    &\quad
    \text{As $f$ is convex, if the hessian is positive semidefinite for
any $\vec{z}$}\nonumber \\ & \quad
    \text{the found local minimum $\vec{x}^*$ is global:}
    \nonumber\\ & \qquad
    \begin{aligned}
        \mathbf H(\vec{z}) \text{ positive semidefinite } \forall \vec{z}
        & \Rightarrow & \vec{x}^*\text{ is a global minimum}
    \end{aligned}
\end{align}



\newpage

\end{document}
